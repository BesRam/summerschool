{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97ec18a",
   "metadata": {},
   "source": [
    "# üîç Multi-Agent Augmented Analytics Workflow\n",
    "This notebook uses a Large Language Model (LLM) to simulate a multi-agent\n",
    "system for analyzing data on global shark attacks. The dataset is processed\n",
    "in chunks using the following specialized agents:\n",
    "\n",
    "üß© Data Ingestion Agent  \n",
    "Reviews the dataset schema, column types, and suggests preprocessing steps.\n",
    "\n",
    "üìä Statistics Agent  \n",
    "Performs statistical analysis: mean, median, distributions, and correlations.\n",
    "\n",
    "ü§ñ ML Agent  \n",
    "Recommends appropriate machine learning models and feature engineering\n",
    "strategies for predicting car prices.\n",
    "\n",
    "üìà Insight Generation Agent  \n",
    "Extracts actionable business insights and trends from the data for\n",
    "decision-making.\n",
    "\n",
    "Each agent analyzes a portion of the dataset and their insights are later\n",
    "aggregated to provide a comprehensive understanding of the full dataset.\n",
    "\n",
    "üí° What are Chunks?  \n",
    "Chunks are small, manageable subsets of the dataset processed independently\n",
    "by agents. They enable parallel processing, modularity, and scalability.\n",
    "\n",
    "üí° Why Use Chunks?  \n",
    "Chunking allows different agents to analyze data simultaneously, speeding up\n",
    "processing and supporting distributed reasoning over large datasets.\n",
    "\n",
    "üí° How is Aggregation Done?  \n",
    "After each agent analyzes its chunk, results such as means, variances, or\n",
    "model outputs are combined using statistical methods (e.g., weighted averages,\n",
    "merged distributions) to produce global insights across the full dataset.\n",
    "\n",
    "Data Source: https://www.kaggle.com/datasets/mexwell/global-shark-attack?resource=download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a0977",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load OpenAI API key from credentials.json\n",
    "try:\n",
    "    with open('../credentials.json', encoding='utf-8') as file:\n",
    "        credentials = json.load(file)\n",
    "        API_KEY = credentials['openai']['api_key']\n",
    "except FileNotFoundError as exc:\n",
    "    raise ValueError(\n",
    "        \"Please provide OpenAI API key in the credentials.json file.\"\n",
    "    ) from exc\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Show currect working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7367ba",
   "metadata": {},
   "source": [
    "## Multi-Agent Augmented Analytics Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc774d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIG ==========\n",
    "CHUNK_SIZE = 5 # Number of rows per chunk\n",
    "DATA_PATH = \"data/global-shark-attack.csv\"\n",
    "\n",
    "# ========== SETUP ==========\n",
    "columns = ['Date','Year','Type','Country','Area','Location','Activity',\n",
    "           'Name','Sex','Age','Injury','Fatal', 'Time','Species']\n",
    "df = pd.read_csv(DATA_PATH, sep=\";\")[columns].head(10) # first 10 rows for demo\n",
    "chunks = [df[i:i + CHUNK_SIZE] for i in range(0, len(df), CHUNK_SIZE)]\n",
    "\n",
    "# ========== AGENTS ==========\n",
    "agents = [\n",
    "    {\n",
    "        \"name\": \"Data Ingestion Agent\",\n",
    "        \"role\": \"data ingestion expert\",\n",
    "        \"instruction\": (\n",
    "            \"Describe the data schema, column types, and any preprocessing suggestions.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Statistics Agent\",\n",
    "        \"role\": \"statistical analyst\",\n",
    "        \"instruction\": (\n",
    "            \"Analyze summary statistics and correlation (if applicable). Identify trends and anomalies.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ML Agent\",\n",
    "        \"role\": \"machine learning engineer\",\n",
    "        \"instruction\": (\n",
    "            \"Evaluate model performance. Suggest improvements or highlight predictive variables.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Insight Generation Agent\",\n",
    "        \"role\": \"business analyst\",\n",
    "        \"instruction\": (\n",
    "            \"Generate actionable insights from this data chunk. Focus on trends relevant to decision-makers.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "# ========== GPT CALLER ==========\n",
    "def call_agent(agent_role, agent_instruction, data_sample):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a {agent_role}.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"{agent_instruction}\\n\\nHere is the relevant data:\\n\"\n",
    "                f\"{data_sample}\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=messages,\n",
    "            max_tokens=1000,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error calling agent: {str(e)}\"\n",
    "\n",
    "# ========== HELPER: STATS ==========\n",
    "def generate_statistics_text(df_chunk):\n",
    "    num_cols = ['Age', 'Year']\n",
    "    df_chunk['Age'] = pd.to_numeric(df_chunk['Age'], errors='coerce')\n",
    "    stats = df_chunk[num_cols].describe().round(2).to_string()\n",
    "    corr = df_chunk[num_cols].corr().round(2).to_string()\n",
    "    return f\"Descriptive Statistics:\\n{stats}\\n\\nCorrelation Matrix:\\n{corr}\"\n",
    "\n",
    "# ========== HELPER: ML ==========\n",
    "def train_rf_model(df_chunk):\n",
    "    df_chunk['Age'] = pd.to_numeric(df_chunk['Age'], errors='coerce')\n",
    "\n",
    "    # Select features and mock target: \"Type\" (Unprovoked vs Provoked, etc.)\n",
    "    feature_cols = ['Country', 'Activity', 'Sex', 'Age']\n",
    "    target_col = 'Type'\n",
    "\n",
    "    df_model = df_chunk[feature_cols + [target_col]].dropna()\n",
    "\n",
    "    if len(df_model) < 10:\n",
    "        return \"Insufficient data for model training in this chunk.\"\n",
    "\n",
    "    X_cat = df_model[['Country', 'Activity', 'Sex']]\n",
    "    X_num = df_model[['Age']]\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    X_encoded = encoder.fit_transform(X_cat)\n",
    "    X = np.hstack([X_encoded, X_num.values])\n",
    "    y = df_model[target_col].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, zero_division=0)\n",
    "    return (\n",
    "        f\"Random Forest Classifier trained on {len(X)} samples.\\n\"\n",
    "        f\"Target: Type (e.g., Unprovoked/Provoked)\\n\\nClassification Report:\\n{report}\"\n",
    "    )\n",
    "\n",
    "# ========== RUN AGENTS ==========\n",
    "all_results = {agent['name']: [] for agent in agents}\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nüîÑ Processing Chunk {i+1}/{len(chunks)}\")\n",
    "\n",
    "    for agent in agents:\n",
    "        print(f\"\\n-- {agent['name']} on Chunk {i+1} --\")\n",
    "\n",
    "        if agent['name'] == \"Statistics Agent\":\n",
    "            data_sample = generate_statistics_text(chunk)\n",
    "        elif agent['name'] == \"ML Agent\":\n",
    "            data_sample = train_rf_model(chunk)\n",
    "        else:\n",
    "            data_sample = chunk.head(10).to_csv(index=False)\n",
    "\n",
    "        output = call_agent(\n",
    "            agent_role=agent['role'],\n",
    "            agent_instruction=agent['instruction'],\n",
    "            data_sample=data_sample\n",
    "        )\n",
    "\n",
    "        all_results[agent['name']].append(output)\n",
    "        print(output)\n",
    "\n",
    "# ========== AGGREGATE RESULTS ==========\n",
    "def aggregate_outputs(agent_name, outputs):\n",
    "    summary_input = \"\\n\\n---\\n\\n\".join(outputs)\n",
    "    return call_agent(\n",
    "        agent_role=f\"{agent_name} summarizer\",\n",
    "        agent_instruction=(\n",
    "            f\"Summarize the following outputs from {agent_name} across all data chunks.\"\n",
    "        ),\n",
    "        data_sample=summary_input[:30000]\n",
    "    )\n",
    "\n",
    "print(\"\\n\\nüîé Final Aggregated Results\\n===========================\")\n",
    "\n",
    "for agent in agents:\n",
    "    print(f\"\\nüìå {agent['name']}\")\n",
    "    summary = aggregate_outputs(agent['name'], all_results[agent['name']])\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8004980",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss25env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
